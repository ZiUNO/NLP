\documentclass[UTF8]{ctexart}
\title{A Neural Probabilistic Language Model}
\author{ZiUNO}
\date{}
\usepackage{amsmath}
\usepackage{graphicx}
\usepackage{fancyhdr}
\pagestyle{fancy}
\lhead{\author}
\chead{\date}
\rhead{ZiUNO}
\lfoot{}
\cfoot{\thepage}
\rfoot{}
\renewcommand{\headrulewidth}{0.4pt}
\renewcommand{\headwidth}{\textwidth}
\renewcommand{\footrulewidth}{0pt}
\usepackage{setspace}
\onehalfspacing
\begin{document}
\maketitle
\begin{abstract}
基于统计学的语言模型的目标是获得词序列的联合概率分布。然而，由于维度诅咒的存在，这在本质上十分困难，因此本文尝试从问题自身寻找解决方法。在本文内容中，同时解决了（1）词分布表示和（2）词序列概率分布问题。本文在两个文本数据集上进行实验验证，相较于最先进的三元语言模型，本文中使用神经网络的概率分布，效果取得显著提升。
\end{abstract}
\section{引言}
\par{在词序列中，给定前面的词的情况下，下一个词的条件概率可用于表示基于统计学的语言模型，即$P(w_{1}^T)=\prod_{t=1}^TP(w_{t}|w_{1}^{t-1})$。其中，$w_{t}$表示第$t$个词，子序列$w_{i}^j=(w_{i},w_{i+1},\dots,w_{j-1},w_{j})$。}
\par{事实上，在词序列中，顺序近的词在统计学上更具有依赖性。因此，对于每个上下文，对下一个词的概率分布采用$n-gram$模型。例如，结合前$n-1$个词，则$P(w_{t}|w_{1}^{t-1})\approx P(w_{t}|w_{t-n+1}^{t-1})$。然而，这只适用于在训练集中出现或词频高的词组合，对于在训练集中未出现的$n$个词的新组合，可以简单地通过使用更小的上下文来解决，如退化三元模型或者插值三元模型。针对从训练集中出现的词序列推广到新的词序列问题，可以在足够短的上下文中，例如，将训练集中词频高的长度为1、2或3的词片段进行“粘合”，并获取该长序列的概率。然而，这种情况下存在以下缺陷：}
\begin{enumerate}
  \item 未考虑长度远远超过1或2个词的上下文
  \item 未考虑词语之间的相似性
\end{enumerate}
\end{document}
