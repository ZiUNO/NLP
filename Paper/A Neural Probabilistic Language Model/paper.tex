\documentclass[UTF8]{ctexart}
\title{A Neural Probabilistic Language Model}
\author{ZiUNO}
\date{}
\usepackage{amsmath}
\usepackage{graphicx}
\usepackage{fancyhdr}
\pagestyle{fancy}
\lhead{\author}
\chead{\date}
\rhead{ZiUNO}
\lfoot{}
\cfoot{\thepage}
\rfoot{}
\renewcommand{\headrulewidth}{0.4pt}
\renewcommand{\headwidth}{\textwidth}
\renewcommand{\footrulewidth}{0pt}
\usepackage{setspace}
\onehalfspacing
\begin{document}
\maketitle
\begin{abstract}
基于统计学的语言模型的目标是获得词序列的联合概率分布。然而，由于维度诅咒的存在，这在本质上十分困难，因此本文尝试从问题自身寻找解决方法。在本文内容中，同时解决了（1）词分布表示和（2）词序列概率分布问题。本文在两个文本数据集上进行实验验证，相较于最先进的三元语言模型，本文中使用神经网络的概率分布，效果取得显著提升。
\end{abstract}
\section{引言}
\par{在词序列中，给定前面的词的情况下，下一个词的条件概率可用于表示基于统计学的语言模型，即$P(w_{1}^T)=\prod_{t=1}^TP(w_{t}|w_{1}^{t-1})$。其中，$w_{t}$表示第$t$个词，子序列$w_{i}^j=(w_{i},w_{i+1},\cdots,w_{j-1},w_{j})$。}
\par{事实上，在词序列中，顺序近的词在统计学上更具有依赖性。因此，对于每个上下文，对下一个词的概率分布采用$n-gram$模型，例如，结合前$n-1$个词，则$P(w_{t}|w_{1}^{t-1})\approx P(w_{t}|w_{t-n+1}^{t-1})$。然而，这只适用于在训练集中出现或词频高的词组合，对于在训练集中未出现的$n$个词的新组合，可以简单地通过使用更小的上下文来解决，如退化三元模型或者插值三元模型。针对从训练集中出现的词序列推广到新的词序列问题，可以在足够短的上下文中，例如，将训练集中词频高的长度为1、2或3的词片段进行“粘合”，并获取该长序列的概率。然而，这种情况下存在以下缺陷：}
\begin{enumerate}
  \item 未考虑长度远远超过1或2个词的上下文；
  \item 未考虑词语之间的相似性。
\end{enumerate}
\subsection{解决维度诅咒}
本文方法思想总结如下：
\begin{enumerate}
  \item 将单词表中每个词映射到一个分布“特征向量”上，因此可生成词间相似度；
  \item 使用词的特征向量表示词序列的联合概率分布；
  \item 同时获得词特征向量和分布中的参数。
\end{enumerate}
\section{两种架构}
\par{训练集为词序列$w_{1}\cdots w_{T}$（$w_{t}\in V$），其中，$V$表示有限词汇集。本文目标为获取模型满足$f(w_{t},\cdots,w_{t-n})=\widehat{P}(w_{t}|w_{1}^{t-1})$，并使得该模型对样本外数据可以得到高似然率。实验中使用$1/\widehat{P}(w_t|w_{1}^{t-1})$的几何平均表示困惑度（perplexity），即平均负对数似然率的指数。模型的唯一限制条件为：$\forall w_{1}^{t-1}, \sum _{i=1}^{|V|}f(i,w_{t-1},w_{t-n})=1$。}
\par{模型基础构成如上所述。为了该模型加速和推广模型将进行如下改良，接下来将$f(w_{t},\cdots,w_{t-n})=\widehat{P}(w_{t}|w_{1}^{t-1})$分为两部分：}
\begin{enumerate}
  \item 映射$C:V\rightarrow C(i)$，其中，$C(i)\in R^{m}$，$C$的大小为$|V| \times m$。
  \item 使用$C$表示的词概率分布。以下为两种构想：
  \begin{enumerate}
    \item 直连架构：根据以前的词得到下一个词的概率分布。
    \begin{enumerate}
      \item $input:(w_{t-n},\cdots,w_{t-1})$
      \item $C(i):(C(w_{t-n}),\cdots,C(w_{t-1}))$
      \item $h_{i}:h(i,C(w_{t-1}),\cdots,C(w_{t-n}))$
      \item $output:softmax(h)_{i}$
    \end{enumerate}
    \item 循环架构：根据当前所有词（下一个词为所有可能词）获得概率分布。
    \begin{enumerate}
      \item $input:(w_{t-n},\cdots,w_{t-1},i)$
      \item $C(i):(C(w_{t-n}),\cdots,C(w_{t-1}),C(i))$
      \item $h_{i}:h(C(i),C(w_{t-1}),\cdots,C(w_{t-n}))$
      \item $output:softmax(h)_{i}$
    \end{enumerate}
  \end{enumerate}
\end{enumerate}
\par{$maximize:L=\frac{1}{T}\sum_{t}\log p_{w_{t}}(C(w_{t-n}),\cdots,C(w_{t-1});\theta)+R(\theta,C)$}
\par{$result:(\theta,C)$}
\section{改良和其他技巧}
\paragraph{短列表}
将最高概率的词组成“短列表”（$L_{t}$）。短列表可使用插值三元语言模型获得，包含最可能出现的下一词，与前两个词相关联。
$$
P(w_{t}=i|h_{t})=
\begin{cases}
\widehat{P}_{NN}(w_{t}=i|w_{t}\in L_{t},h_{t})\widehat{P}_{trigram}(w_{t}\in L_{t}|h_{t}) & {i\in L_{t}}\\
P_{trigram}(w_{t}=i|h_{t}) & {i\notin L_{t}}
\end{cases}
$$
\par{其中，$\widehat{P}_{NN}(w_{t}=i|w_{t}\in L_{t},h_{t})$表示归一化得分（$softmax$仅对$L_{t}$中的词进行归一化），$\widehat{P}_{trigram}(w_{t}\in L_{t}|h_{t})=\sum_{i\in L_{t}}\widehat{P}_{trigram}(i|h_{t})$，$\widehat{P}_{trigram}(i|h_{t})$表示使用插值三元模型计算得到的下一个词的概率。}
\paragraph{查表识别}
将预计算的神经网络的输出保存在哈希表中。
\paragraph{SGD}
\paragraph{容量控制}
\paragraph{词向量初始化}
\begin{enumerate}
  \item $[-0.01,0.01]$内的随机值。
  \item 获得共现矩阵，并计算SVD。
\end{enumerate}
\paragraph{词汇表外词语}
使用短列表中所有单词的加权平均特征向量作为特征向量。
\section{实验结果}
\par{实验中主要的结果表明：神经网络比平滑三元模型表现得更好。}
\section{问题}
\begin{enumerate}
  \item The Proposed Model: two Architectures中的两种架构构想中的$g$和$h$有点混乱。
  \item $L=\frac{1}{T}\sum_{t}\log p_{w_{t}}(C(w_{t-n}),\cdots,C(w_{t-1});\theta)+R(\theta,C)$，其中$\sum_{t}$下方的$t$是否表示所有真值？
  \item 词汇表外词语的相对概率用短列表的权重表示？
  \item 如果下一词为词汇表外词语？
\end{enumerate}
\end{document}
